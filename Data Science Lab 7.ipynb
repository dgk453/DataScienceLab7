{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Science Lab: Lab 7 part I (of III)\n",
        "\n",
        "Submit:\n",
        "\n",
        "A pdf of your notebook with solutions.\n",
        "A link to your colab notebook or also upload your .ipynb if not working on colab.\n",
        "\n",
        "# Goals of this Lab\n",
        "\n",
        "There are a number of goals of this notebook:\n",
        "\n",
        "1.   Learning to put together a basic model beyond what we did in the previous notebook. Main emphasis: new layers, including convolution and max pooling.\n",
        "2.   Learning the basics of training.\n",
        "3.   Learning basics of loading data and visualizing.\n",
        "4.   Learning the basics of printing out a model.\n",
        "\n",
        "Also useful to see\n",
        "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/2510/1*vkQ0hXDaQv57sALXAJquxA.jpeg\" width=1024px/>"
      ],
      "metadata": {
        "id": "JI3k8xslN-dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "# the tutorial calls the dataloader twice -- this code defines a function\n",
        "# that will do this for the train/test data.\n",
        "\n",
        "def fetch_dataloader(batch_size, transform=None, is_train=True):\n",
        "    \"\"\"\n",
        "    We saw the data loaders in the previous lab.\n",
        "    This creates a method for us to get (image, label) pairs.\n",
        "    We can use it so that we do not have to load everything into memory\n",
        "    at once.\n",
        "    \"\"\"\n",
        "    data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "    # Custom train/val split.\n",
        "    indices = [i for i in range(len(data)) if (i%10 > 0) == is_train]\n",
        "\n",
        "    data = torch.utils.data.Subset(data, indices)\n",
        "    loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    return loader\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "val_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "data_train = fetch_dataloader(batch_size, train_transform, is_train=True)\n",
        "data_val = fetch_dataloader(batch_size, val_transform, is_train=False)"
      ],
      "metadata": {
        "id": "8p9fen4rODkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1 (nothing to turn in)\n",
        "\n",
        "Read about transforms. These are routinely used when loading images. We've used a particular set of parameters. But you will see many other choices.\n",
        "\n",
        "https://pytorch.org/vision/stable/transforms.html\n"
      ],
      "metadata": {
        "id": "_8Rrhc9mYOCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2\n",
        "\n",
        "Figure out how to use the data loader to display the data.\n",
        "For example, you could look here\n",
        "\n",
        "https://towardsdatascience.com/beginners-guide-to-loading-image-data-with-pytorch-289c60b7afec\n",
        "and then https://stackoverflow.com/questions/51756581/how-do-i-turn-a-pytorch-dataloader-into-a-numpy-array-to-display-image-data-with\n",
        "\n",
        "or you could look at the pytorch CIFAR10 tutorial\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
      ],
      "metadata": {
        "id": "xfBSB388YjYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Implementation.\n",
        "\n",
        "Now it is time to implement our CNN. You will recognize syntax from the previous Lab. However, whereas in the last lab we had only fully connected layers and ReLU layers, here we will use more options. In addition to fully connected and ReLU layers, we want to use:\n",
        "\n",
        "* Convolutional layers: `torch.nn.Conv2d`\n",
        "* Max Pooling Layers:`torch.nn.MaxPool2d`\n",
        "* Average Pooling Layers: `torch.nn.AvgPool2d`\n",
        "\n",
        "Fully connected layers primarily had 3 parameters: input size, output size, and bias.\n",
        "\n",
        "Convolutional layers have many more parameters, as we discussed in class, In particular, recall:\n",
        "\n",
        "* kernel_size\n",
        "* stride\n",
        "* padding\n",
        "* dilation\n",
        "\n",
        "Read about these in the Pytorch documentation:\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n"
      ],
      "metadata": {
        "id": "3sgcXSTLOWDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we see some basic code, similar in spirit to our previous colab notebook, but, critically, adding a new type of layer: a convolutional layer."
      ],
      "metadata": {
        "id": "XsG9L6AUR_S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We define two different models, using different syntax.\n",
        "# CNNClassifier is basic code borrowed from Phil Krahenbuhl\n",
        "# ConvNet comes from the pytorch tutorial linked above.\n",
        "#\n",
        "\n",
        "class CNNClassifier(torch.nn.Module):\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "        \"\"\"\n",
        "        Define the layer(s) needed for the model.\n",
        "        Feel free to define additional input arguments.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(input_channels, 16, 7, 2, 3)\n",
        "        self.cls = nn.Linear(16, num_classes)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        # or could do\n",
        "        # self.CNN = nn.Sequential(torch.nn.Conv2d(input_channels, 16, 7, 2, 3), torch.nn.ReLU(),torch.nn.Linear(16, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Calculate the classification score (logits).\n",
        "\n",
        "        Input:\n",
        "            x (float tensor N x 3 x 32 x 32): input images\n",
        "        Output:\n",
        "            y (float tensor N x 10): classification scores (logits) for each class\n",
        "        \"\"\"\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # Add a ReLU\n",
        "        x = self.ReLU(x)\n",
        "        # Add global average pooling\n",
        "        x = x.mean(dim=(2,3))\n",
        "        return self.cls(x)\n",
        "\n",
        "    def predict(self, image):\n",
        "        return self(image).argmax(1)\n",
        "\n",
        "# From the pytorch tutorial linked above.\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# We can rewrite it equivalently this way\n",
        "class ConvNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# This way is also equivalent\n",
        "class ConvNet3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mfX01eXCOSPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training and Validation\n"
      ],
      "metadata": {
        "id": "ADZBQqs0O_Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we train the first model.\n",
        "#\n",
        "\n",
        "\n",
        "# Choose a model to train\n",
        "model = CNNClassifier(3, 10)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "\n",
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(data_train, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "UUwTRj3QD_Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see how well this trained model performs on a couple data points.\n",
        "dataiter = iter(data_val)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "plt.imshow(torchvision.utils.make_grid(images).permute(1,2,0))\n",
        "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "\n",
        "# and now what our model thinks\n",
        "output = model(images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
        "                              for j in range(4)))"
      ],
      "metadata": {
        "id": "QBpMcMooGGgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3\n",
        "\n",
        "Use your validation data loader (data_val) to assess the accuracy on the entire data set.\n",
        "\n",
        "If you did this correctly, you should find that your accuracy is somewhere in the 30-40% range -- not great, but a lot better than guessing, and much better than we were able to do with trees.  "
      ],
      "metadata": {
        "id": "zOwztLMgnZMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time to train the deeper model"
      ],
      "metadata": {
        "id": "8h45uhZ_oBvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we train the second model. We see that it does quite a bit better than the first.\n",
        "#\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Choose a model to train\n",
        "model2 = ConvNet() # note that this command resets the parameters\n",
        "model2.to(device)\n",
        "model2.train()\n",
        "\n",
        "# Set the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model2.parameters(), lr=0.001, momentum=0.9)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7fTVlT9R4kuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(data_train, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model2(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "Si8PVtbbCYgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4\n",
        "\n",
        "Repeat what you did for Problem 3 above: use your validation data loader (data_val) to assess the accuracy on the entire data set.\n",
        "\n",
        "If you did this correctly, you should find that your accuracy is somewhere in the 50-60% range. Far from perfect, but still better than how we did above."
      ],
      "metadata": {
        "id": "eF6mZfy-oT0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 5\n",
        "\n",
        "How many parameters does your model have? You can either compute by hand, or much better, use the summary command from torchsummary\n",
        "```\n",
        "from torchsummary import summary\n",
        "```\n",
        "Note that you'll have to figure out how to use use, and also you will have to enter the size of your input.\n",
        "\n",
        "Also try using the command\n",
        "\n",
        "```\n",
        "for name, layer in model2.named_modules():\n",
        "    print(name,layer)\n",
        "```\n",
        "This one is very useful when we want to download someone else's netework, and we need to know what the names of the layers are."
      ],
      "metadata": {
        "id": "xChhktbAot6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 6\n",
        "\n",
        "How much can you decrease the error? Try to make your own convolutional neural network for CIFAR-10. Don't make it so big that you can't train it on Colab! You can play around with the parameters of the network and also with the parameters of training (train for longer -- more epochs, possibly using other parameters for step size, momentum, etc.).\n",
        "\n",
        "**We will give 20 lab bonus points to the team with the best accuracy.**"
      ],
      "metadata": {
        "id": "C4NiFZjIrKFD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kyo975N8RsVl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}