{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBqCA4iQPbI1"
   },
   "source": [
    "# Data Science Lab: Lab 7 part II (of III)\n",
    "\n",
    "Submit:\n",
    "\n",
    "A pdf of your notebook with solutions.\n",
    "A link to your colab notebook or also upload your .ipynb if not working on colab.\n",
    "\n",
    "## Goals of this Lab: Transfer Learning\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "In the previous notebook, we learned how to put together various different types of layers to build a convolutional neural network (CNN) to classify CIFAR-10. Now we are going to learn about the principle of transfer learning.\n",
    "\n",
    "We will create 2 data sets: one with the first five labels of CIFAR-10 (plane, car, bird, cat, deer), and one with the remaining 5. We will see how training on the second data set, can actually help us with the first.\n",
    "\n",
    "Note: because CIFAR-10 is a relatively small data set, the numbers are not overly compelling. But we can greatly amplify this if we use larger data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FD3Cc4D4s3tm"
   },
   "source": [
    "## Problem 1.\n",
    "\n",
    "Make train and validation (testing) data loaders for the first five labels, and the second five labels.\n",
    "\n",
    "You should have 4 dataloaders when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nmgaz7XPd3p"
   },
   "outputs": [],
   "source": [
    "# First we download the data. This is quite similar to what we've seen before.\n",
    "# The main difference will be that we need to make two different loaders,\n",
    "# one for the first five labels, and one for the remaining five.\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# the tutorial calls the dataloader twice -- this code defines a function\n",
    "# that will do this for the train/test data.\n",
    "\n",
    "def fetch_dataloader(batch_size, label_subset,transform=None, is_train=True):\n",
    "    \"\"\"\n",
    "    Loads data from disk and returns a data_loader.\n",
    "    A DataLoader is similar to a list of (image, label) tuples.\n",
    "    You do not need to fully understand this code to do this assignment, we're happy to explain though.\n",
    "    \"\"\"\n",
    "    data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    # TO DO\n",
    "    # TO DO\n",
    "    # TO DO\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    return loader\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCslTG6BpiAv"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TO DO\n",
    "\n",
    "Complete these four using the function you wrote above, to get the 4 data loaders you need:\n",
    "two for labels 1-5, and two for labels 6-10.\n",
    "\"\"\"\n",
    "data15_train =\n",
    "data15_val =\n",
    "data610_train =\n",
    "data610_val ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03GxoPpWppq5"
   },
   "outputs": [],
   "source": [
    "# Now let's visualize a few images, to see if this worked.\n",
    "# Same exact syntax as in the previous notebook.\n",
    "# Nothing to do here, except run it to make sure it worked.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "%matplotlib inline\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(data610_train)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKP1etNjqMKu"
   },
   "outputs": [],
   "source": [
    "# We define the same model from the last notebook\n",
    "# that we got from the Pytorch ConvNet tutorial.\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Et9ZJIiHrd-e"
   },
   "outputs": [],
   "source": [
    "# We train the model on the first five labels to see how well we do after a fixed amount of training.\n",
    "#\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Choose a model to train\n",
    "model1 = ConvNet(5) # note that this command resets the parameters\n",
    "model1.to(device)\n",
    "model1.train()\n",
    "\n",
    "# Set the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr=0.001, momentum=0.9)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "430y8iAvHNVj"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1):  # a single pas through the data. Don't change this.\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(data15_train, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model1(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQAv9abvtb0t"
   },
   "source": [
    "## Problem 2\n",
    "\n",
    "In the cell above, you trained your model on labels 1-5 for one single epoch.\n",
    "\n",
    "How well does this do on the validation data set?\n",
    "\n",
    "This is nearly identical to problems you solved in part I.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrpe3TOBHz-2"
   },
   "outputs": [],
   "source": [
    "# Now we train the model on the last 5 labels to use for transfer learning.\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Choose a model to train\n",
    "model2 = ConvNet(5) # note that this command resets the parameters\n",
    "model2.to(device)\n",
    "model2.train()\n",
    "\n",
    "# Set the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=0.001, momentum=0.9)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfG6gnF7IN4r"
   },
   "outputs": [],
   "source": [
    "for epoch in range(6):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(data610_train, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        labels = labels-5 # hack b/c labels are 2-9 instead of 0-7\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model2(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 500 == 499:    # print every 500 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 500))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXSl1m-ZJ0Au"
   },
   "source": [
    "Now we see how we can use this model for the first 5 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euRkOEpZIduy"
   },
   "outputs": [],
   "source": [
    "# We define a new model to fine-tune\n",
    "model3 = model2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SPHjnftt2l3"
   },
   "source": [
    "We have a new model: model3. This model has the same weights as model2, hence it is good at classifying labels, 6-10, and does not know about labels 1-5. We will have to reset and retrain the last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAJJZ9fYt7pI"
   },
   "source": [
    "## Problem 3\n",
    "\n",
    "Let's pretend that didn't build model3 ourselves. In this case, to reset the last layer, we would need to figure out what its name is (we know it's fc3, but we're pretending we don't).\n",
    "\n",
    "Find a command that will tell you the name and type of the last layer.  Print them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyOVUkvzuRgX"
   },
   "source": [
    "## Problem 4\n",
    "\n",
    "If we did the above correctly, then we learned that the last layer is called fc3 and it is a linear layer.\n",
    "\n",
    "We want to redefine it giving a command like\n",
    "\n",
    "```\n",
    "model3.fc3 = nn.Linear(input_features,output_features)\n",
    "```\n",
    "\n",
    "* What should be the value of output_features for our new last layer that we are defining? The answer to this comes from the problem we want to solve.\n",
    "\n",
    "* What is the number of input features that go into this last layer? You'll have to look at the size of the layer defined in the network. Again, pretend we didn't make the network, and we just downloaded it.\n",
    "\n",
    "So in other words, I'm asking you to use commands to find out how many input features there are for the last layer which (we know from above) is called fc3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKswU-QLvemW"
   },
   "source": [
    "## Problem 5\n",
    "\n",
    "Now redefine the last layer. You will do this with a command like\n",
    "```\n",
    "model3.fc3 = # TO DO\n",
    "```\n",
    "Note how important it is for us to know what the names of the layers are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnF-H517u6LD"
   },
   "outputs": [],
   "source": [
    "model3.fc3 = # TO DO\n",
    "model3 = model3.to(device)\n",
    "model3.train()\n",
    "\n",
    "# Let's again check the layers and see the change we made\n",
    "# (doesn't actually look different since we split 5 and 5)\n",
    "for name, layer in model2.named_modules():\n",
    "  print(name,layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vV-9G6kCTmsK"
   },
   "outputs": [],
   "source": [
    "# Set the optimizer (loss function already set)\n",
    "optimizer = torch.optim.SGD(model3.parameters(), lr=0.001, momentum=0.9)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00YXXudtvhFi"
   },
   "source": [
    "## Problem 6\n",
    "\n",
    "Check that model3 is no good for classes 1-5. (It shouldn't be any good! We just added a randomly initialized last layer). The accuracy should be close to 20%, which is how good random guessing would be for 5 labels.\n",
    "\n",
    "Evaluate its accuracy on the validation set for labels 1-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b75wMgYSOIw8"
   },
   "outputs": [],
   "source": [
    "# Now we train model3 on the first five labels, and evaluate accuracy.\n",
    "# Nothing for you to do here but run this cell.\n",
    "# Note that we are also just using one single pass, to make it a fair\n",
    "# comparison, and so that we can try to see the benefit of transfer learning.\n",
    "#\n",
    "\n",
    "for epoch in range(1):  # one single pass -- don't change this\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(data15_train, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model3(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK3cUPhbwAww"
   },
   "source": [
    "## Problem 7\n",
    "\n",
    "Evaluate the accuracy of model3 (after 1 epoch of training) on the validation set. Compare with the accuracy you found above in Problem 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdQQhtYywRJh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
